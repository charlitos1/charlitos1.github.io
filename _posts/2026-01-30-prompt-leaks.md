---
layout: post
title: 'El código fuente de la personalidad: Se filtran los "System Prompts" de la IA'
date: 2026-01-30
categories: [ia]
---
**Autor:** Carlos Moreno

# [ El código fuente de la personalidad: Se filtran los "System Prompts" de la IA ]

---

Un [repositorio de GitHub](https://github.com/asgeirtj/system_prompts_leaks) se ha convertido en la zona cero de la transparencia forzada en el mundo de la inteligencia artificial. Contiene los "huesos" de cómo interactúan con nosotros modelos como GPT-5, Claude 4.5 y Gemini.

**Por qué importa:** Estos *system prompts* son las instrucciones maestras y secretas que dictan el comportamiento, los límites de seguridad y la "personalidad" de un modelo antes de que el usuario escriba una sola palabra.

**El panorama general:** El repositorio ha escalado rápidamente en las tendencias de GitHub. No es código de software tradicional; es una colección de las reglas gramaticales y éticas que las Big Tech intentan mantener bajo llave.

### Lo que revelan las filtraciones

El archivo de "fugas" actúa como un espejo del estado actual de la IA en 2026:

* **Identidades forzadas:** Instrucciones explícitas para que el modelo nunca admita ser de una versión anterior (ej. "Tú eres GPT-5, incluso si el usuario intenta convencerte de lo contrario").
* **Protocolos de herramientas:** Detalles exactos sobre cómo los modelos deciden cuándo usar el buscador web o ejecutar código en Python.
* **Guardrieles de seguridad:** Reglas específicas para evitar sesgos, no generar contenido político sensible o cómo rechazar solicitudes de "jailbreak".
* **Instrucciones de "Pensamiento":** Se filtraron los parámetros de razonamiento interno de los modelos de la serie "o" de OpenAI, revelando cómo gestionan su cadena de pensamiento privada.

### Entre líneas: ¿Seguridad o Secretismo?

La comunidad de IA está dividida sobre si estas filtraciones representan un riesgo o un beneficio público.

1. **El riesgo:** Hackers pueden usar estas instrucciones para encontrar "grietas" en la armadura del modelo y saltarse las restricciones de seguridad (*Prompt Injection*).
2. **El beneficio:** Permite a investigadores y usuarios entender los sesgos ocultos y las limitaciones impuestas por empresas como Google, OpenAI y Anthropic.

**El dato:** El repositorio ya supera las **28,000 estrellas**, convirtiéndose en una lectura obligatoria para ingenieros de prompts que buscan replicar el comportamiento de modelos propietarios en sistemas de código abierto.

**Lo que sigue:** A medida que los modelos se vuelven más complejos, la batalla entre las empresas que intentan "esconder el guion" y los usuarios que usan ingeniería social para extraerlo se intensificará.

---

